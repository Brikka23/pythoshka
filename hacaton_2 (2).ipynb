{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ca1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import os \n",
    "import collections\n",
    "import subprocess\n",
    "from docx.shared import RGBColor\n",
    "import nltk\n",
    "import sklearn\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import urllib\n",
    "import itertools\n",
    "import tarfile\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "import pickle\n",
    "import re, string, timeit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "russian_stopwords.remove('можно')\n",
    "russian_stopwords.remove('быть')\n",
    "russian_stopwords.remove('или')\n",
    "russian_stopwords.remove('был')\n",
    "russian_stopwords.remove('может')\n",
    "russian_stopwords.remove('нельзя')\n",
    "russian_stopwords.remove('всегда')\n",
    "russian_stopwords.remove('более')\n",
    "russian_stopwords.remove('без')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5307084",
   "metadata": {},
   "outputs": [],
   "source": [
    "highlights_red = collections.defaultdict(list)\n",
    "problem_red = []\n",
    "for region in os.listdir('DataSet_Razmetra'):\n",
    "    if region == 'РСО-Алания':\n",
    "        continue\n",
    "    for corupt_type in os.listdir(os.path.join('DataSet_Razmetra',region)):\n",
    "        for unknown in os.listdir(os.path.join('DataSet_Razmetra',region,corupt_type)):\n",
    "            for ed in os.listdir(os.path.join('DataSet_Razmetra',region,corupt_type, unknown)):\n",
    "                if 'Edition' in ed:\n",
    "                    path = os.path.join('DataSet_Razmetra',region,corupt_type, unknown,ed)\n",
    "                    if os.path.exists(os.path.join(path,'Edition_Text.docx')):\n",
    "                        try:\n",
    "                            doc = os.path.join(path,'Edition_Text.docx')\n",
    "                            document = docx.Document(doc)\n",
    "                            for paragraph in document.paragraphs:\n",
    "                                highlight = \"\"\n",
    "                                for run in paragraph.runs:\n",
    "                                    if run.font.color.rgb == RGBColor(255, 000, 000):\n",
    "                                        highlight += run.text                                \n",
    "                                if highlight:\n",
    "                                    highlight = highlight.replace(u'\\xa0', u' ').replace('\\n', ' ')\n",
    "                                    highlights_red[corupt_type].append(highlight)\n",
    "                        except:\n",
    "                            problem_red.append(doc)\n",
    "                    else:\n",
    "                        doc = os.path.join(path,'Edition_Text.doc')\n",
    "                        problem_red.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1896e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dict = collections.defaultdict(list)\n",
    "problem_docs = []\n",
    "for region in os.listdir('DataSet_Razmetra'):\n",
    "    if region == 'РСО-Алания':\n",
    "        continue\n",
    "    for corupt_type in os.listdir(os.path.join('DataSet_Razmetra',region)):\n",
    "        for unknown in os.listdir(os.path.join('DataSet_Razmetra',region,corupt_type)):\n",
    "            for ed in os.listdir(os.path.join('DataSet_Razmetra',region,corupt_type, unknown)):\n",
    "                if 'Edition' in ed:\n",
    "                    path = os.path.join('DataSet_Razmetra',region,corupt_type, unknown,ed)\n",
    "                    if os.path.exists(os.path.join(path,'NC_Edition_Text.docx')):\n",
    "                        try:\n",
    "                            doc = os.path.join(path,'NC_Edition_Text.docx')\n",
    "                            document = docx.Document(doc)\n",
    "                            for paragraph in document.paragraphs:\n",
    "                                highlight = \"\"\n",
    "                                for run in paragraph.runs:\n",
    "                                    highlight += run.text                                \n",
    "                                if highlight:\n",
    "                                    highlight = highlight.replace(u'\\xa0', u' ').replace('\\n', ' ').replace('\\t', ' ')\n",
    "                                    pure = highlight.lower()\n",
    "                                    pure = re.sub(r'[^\\w\\s]','',pure)\n",
    "                                    words = []\n",
    "                                    for word in pure.split(' '):\n",
    "                                        p = morph.parse(word)[0]\n",
    "                                        words.append(p.normal_form)\n",
    "                                    pure = [w for w in words if not word in russian_stopwords]\n",
    "                                    pure = ' '.join(pure)\n",
    "                                    if pure:\n",
    "                                        docs_dict[doc].append(pure)\n",
    "                        except:\n",
    "                            problem_docs.append(doc)\n",
    "                    else:\n",
    "                        doc = os.path.join(path,'Edition_Text.doc')\n",
    "                        problem_docs.append(doc)\n",
    "with open('data_texts_cor.pickle', 'wb') as f:\n",
    "    pickle.dump(docs_dict, f)\n",
    "    \n",
    "docs_dict_no = collections.defaultdict(list)\n",
    "problem_docs_no = []\n",
    "for region in os.listdir('DataSet_Razmetra'):\n",
    "    if region == 'РСО-Алания':\n",
    "        continue\n",
    "    for corupt_type in os.listdir(os.path.join('DataSet_Razmetra',region)):\n",
    "        for unknown in os.listdir(os.path.join('DataSet_Razmetra',region,corupt_type)):\n",
    "            for ed in os.listdir(os.path.join('DataSet_Razmetra',region,corupt_type, unknown)):\n",
    "                if 'Edition' in ed:\n",
    "                    path = os.path.join('DataSet_Razmetra',region,corupt_type, unknown,ed)\n",
    "                    if os.path.exists(os.path.join(path,'NC_Edition_Text.docx')):\n",
    "                        try:\n",
    "                            doc = os.path.join(path,'NC_Edition_Text.docx')\n",
    "                            document = docx.Document(doc)\n",
    "                            for paragraph in document.paragraphs:\n",
    "                                highlight = \"\"\n",
    "                                for run in paragraph.runs:\n",
    "                                    highlight += run.text                                \n",
    "                                if highlight:\n",
    "                                    highlight = highlight.replace(u'\\xa0', u' ').replace('\\n', ' ').replace('\\t', ' ')\n",
    "                                    pure = highlight.lower()\n",
    "                                    pure = re.sub(r'[^\\w\\s]','',pure)\n",
    "                                    words = []\n",
    "                                    for word in pure.split(' '):\n",
    "                                        p = morph.parse(word)[0]\n",
    "                                        words.append(p.normal_form)\n",
    "                                    pure = [w for w in words if not word in russian_stopwords]\n",
    "                                    pure = ' '.join(pure)\n",
    "                                    if pure:\n",
    "                                        docs_dict_no[doc].append(pure)\n",
    "                        except:\n",
    "                            problem_docs_no.append(doc)\n",
    "                    else:\n",
    "                        doc = os.path.join(path,'Edition_Text.doc')\n",
    "                        problem_docs.append(doc)\n",
    "with open('data_texts_no_cor.pickle', 'wb') as f:\n",
    "    pickle.dump(docs_dict_no, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8237e7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('highlights_red.pickle', 'rb') as f:\n",
    "    highlights_red = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a948b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pickle', 'rb') as f:\n",
    "    highlights = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48e21b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "pure_highlights = {}\n",
    "for i in highlights.keys():\n",
    "    tok = []\n",
    "    for j in highlights[i]:\n",
    "        if re.search('[а-яА-Я]',j):\n",
    "            pure = j.replace(u'\\xa0', u' ').replace('\\n', ' ')\n",
    "            pure = pure.lower()\n",
    "            pure = re.sub(r'[^\\w\\s]','',pure)\n",
    "            words = []\n",
    "            for word in pure.split(' '):\n",
    "                p = morph.parse(word)[0]\n",
    "                words.append(p.normal_form)\n",
    "            pure = [w for w in words if not word in russian_stopwords]\n",
    "            pure = ' '.join(pure)\n",
    "            if pure:\n",
    "                tok.append(pure)\n",
    "        pure_highlights[i] = list(set(tok))\n",
    "with open('pure_highlights.pickle', 'wb') as f:\n",
    "    pickle.dump(pure_highlights, f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8acd2520",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "pure_highlights_red = {}\n",
    "for i in highlights_red.keys():\n",
    "    tok = []\n",
    "    for j in highlights_red[i]:\n",
    "        if re.search('[а-яА-Я]',j):\n",
    "            pure = j.replace(u'\\xa0', u' ').replace('\\n', ' ')\n",
    "            pure = pure.lower()\n",
    "            pure = re.sub(r'[^\\w\\s]','',pure)\n",
    "            words = []\n",
    "            for word in pure.split(' '):\n",
    "                p = morph.parse(word)[0]\n",
    "                words.append(p.normal_form)\n",
    "            pure = [w for w in words if not word in russian_stopwords]\n",
    "            pure = ' '.join(pure)\n",
    "            if pure:\n",
    "                tok.append(pure)\n",
    "        pure_highlights_red[i] = list(set(tok))\n",
    "with open('pure_highlights_red.pickle', 'wb') as f:\n",
    "    pickle.dump(pure_highlights_red, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0938959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dict_label = collections.defaultdict(list)\n",
    "problem_docs_label = []\n",
    "label_dict = {'3_1':1,'3_2':2,'3_3':3,'3_4':4,'3_5':5,'3_6':6,'3_7':7,'3_8':8,'3_9':9,'4_1':10,'4_2':11,'4_4':12}\n",
    "for region in os.listdir('DataSet_Razmetra'):\n",
    "    if region == 'РСО-Алания':\n",
    "        continue\n",
    "    for corupt_type in os.listdir(os.path.join('DataSet_Razmetra',region)):\n",
    "        for unknown in os.listdir(os.path.join('DataSet_Razmetra',region,corupt_type)):\n",
    "            for ed in os.listdir(os.path.join('DataSet_Razmetra',region,corupt_type, unknown)):\n",
    "                if 'Edition' in ed:\n",
    "                    path = os.path.join('DataSet_Razmetra',region,corupt_type, unknown,ed)\n",
    "                    if os.path.exists(os.path.join(path,'NC_Edition_Text.docx')):\n",
    "                        try:\n",
    "                            doc = os.path.join(path,'NC_Edition_Text.docx')\n",
    "                            document = docx.Document(doc)\n",
    "                            docs_dict_label[doc].append(0)\n",
    "                            docs_dict_label[doc].append(0)\n",
    "                        except:\n",
    "                            problem_docs_label.append(doc)\n",
    "                    if os.path.exists(os.path.join(path,'Edition_Text.docx')):\n",
    "                        try:\n",
    "                            doc = os.path.join(path,'Edition_Text.docx')\n",
    "                            document = docx.Document(doc)\n",
    "                            docs_dict_label[doc].append(1)\n",
    "                            docs_dict_label[doc].append(label_dict[corupt_type])\n",
    "                        except:\n",
    "                            problem_docs_label.append(doc)\n",
    "                    else:\n",
    "                        doc = os.path.join(path,'Edition_Text.doc')\n",
    "                        problem_docs_label.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd74aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in docs_dict.values():\n",
    "    for j in i:\n",
    "        corpus.append(j)\n",
    "for i in docs_dict_no.values():\n",
    "    for j in i:\n",
    "        corpus.append(j)\n",
    "for i in highlights.values():\n",
    "    for j in i:\n",
    "        corpus.append(j)\n",
    "for i in higligts_red.values():\n",
    "    for j in i:\n",
    "        corpus.append(j)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71305bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train_simple = []\n",
    "y_train = []\n",
    "import random\n",
    "deom scipy.spatial.distance import cosine\n",
    "for i in sorted(docs_dict.keys()):\n",
    "    tmp = ''\n",
    "    for par in docs_dict[i]:\n",
    "        tmp = tmp + par + ' '\n",
    "    distance_vec = []\n",
    "    for cor in sorted(pure_highlights.keys()):\n",
    "        cor_par = ''\n",
    "        for form in cor:\n",
    "            cor_par = cor_par + form + ' '\n",
    "        sim = cosine(vectorizer.transform([cor_par]), vectorizer.transform([tmp])\n",
    "        distance_vec.append(sim)\n",
    "    X_train.append(distance_vec)\n",
    "    y_train.append(docs_dict_label[i][1])\n",
    "    y_train_simple(docs_dict_label[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a2e8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model from SGDClassifier\n",
    "classifier = SGDClassifier()\n",
    "classifier.fit(X_train,y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
